{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' create datasets '''\n",
    "\n",
    "def create_dataset(col_num):\n",
    "    # create entry dataset X (train or test)\n",
    "    X = np.random.randint(2, size=(2,col_num))\n",
    "    \n",
    "    # create label dataset Y (train or test)\n",
    "    Y = np.sum(X, axis=0, keepdims=True)\n",
    "    Y[Y!=1] = 0\n",
    "\n",
    "    # create noises in the entry dataset X by adding (-0.6, 0.6) to the data\n",
    "    X_noise = np.random.randn(2,col_num)\n",
    "    X_noise = X + (X_noise / 20)\n",
    "    \n",
    "    return X_noise, Y\n",
    "\n",
    "\n",
    "''' initialize parameters -- W, b '''\n",
    "def initialize_parameters(layer_dims):\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    # for the first L-1 layers, we use a heristic to initialize weight that is customized to the relu function\n",
    "    for i in range(1, L):\n",
    "        parameters[f'W{i}'] = np.random.randn(layer_dims[i], layer_dims[i-1]) * relu(None, heuristic=layer_dims[i-1])\n",
    "        parameters[f'b{i}'] = np.zeros((layer_dims[i], 1))\n",
    "    \n",
    "    # for the last layer (L), we use a heristic to initialize weight that is customized to the sigmoid function\n",
    "    parameters[f'W{i}'] = np.random.randn(layer_dims[L-1], layer_dims[L-2]) * sigmoid(None, heuristic=layer_dims[L-2])\n",
    "    parameters[f'b{i}'] = np.zeros((layer_dims[L-1], 1))\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "\n",
    "''' define activation function (sigmoid) and its derivative '''\n",
    "def sigmoid(F, derivative=False, heuristic=False):\n",
    "    \n",
    "    # calculate the derivative of sigmoid\n",
    "    if derivative:\n",
    "        return F * (1 - F) # F = A\n",
    "    \n",
    "    # calculate the heuristic to initialize weight that is customized to the sigmoid function \n",
    "    if heuristic:\n",
    "        return np.sqrt(1 / heuristic)\n",
    "    \n",
    "    # calucate the sigmoid function\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-F)) # F = Z\n",
    "    \n",
    "def relu(F, derivative=False, heuristic=False):\n",
    "    \n",
    "    # calculate the derivative of relu\n",
    "    if derivative:\n",
    "        return 1 * (F > 0) # F = Z\n",
    "    \n",
    "    # calculate the heuristic to initialize weight that is customized to the relu function\n",
    "    elif heuristic:\n",
    "        return np.sqrt(2 / heuristic)\n",
    "    \n",
    "    # calucate the relu function\n",
    "    else:\n",
    "        return F * (F > 0) # F = Z\n",
    "    \n",
    "\n",
    "''' 1. forward propagation function - calculate pre-activation fn (Z) & activation fn (A) ''' \n",
    "    \n",
    "def forward_pass(X, parameters, layer_nums):\n",
    "    \n",
    "    cache = {}\n",
    "    cache['A0'] = X\n",
    "    L = len(layer_nums)\n",
    "    \n",
    "    for i in range(1, L-1):\n",
    "        \n",
    "        # for the first L-1 layers, use relu as an activation function\n",
    "        cache[f'Z{i}'] = np.dot(parameters[f'W{i}'], cache[f'A{i-1}']) + parameters[f'b{i}']\n",
    "        cache[f'A{i}'] = relu(cache[f'Z{i}'])\n",
    "\n",
    "    # for the last layer L, use sigmoid as an activation function\n",
    "    cache[f'Z{L-1}'] = np.dot(parameters[f'W{L-1}'], cache[f'A{L-2}']) + parameters[f'b{L-1}']\n",
    "    cache[f'A{L-1}'] = sigmoid(cache[f'Z{L-1}'])\n",
    "\n",
    "    return cache\n",
    "\n",
    "\n",
    "''' 2. calculate cost '''\n",
    "def cost(A, Y):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    J = - np.sum (Y * np.log(A) + (1 - Y) * np.log(1 - A)) / m\n",
    "\n",
    "    return J\n",
    "\n",
    "\n",
    "''' 3. backward propagation fonction - calculate dW & db from dA & dZ'''\n",
    "\n",
    "# backward non_linear function to calculate dA & dZ\n",
    "def backward_pass(cache, parameters, Y, layer_dims):\n",
    "    \n",
    "    grads = {}\n",
    "    m = Y.shape[1]\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    # for last layer, use the derivative of sigmoid, which is simply (A-Y). So no need to call the sigmoid function\n",
    "    dZ = cache[f'A{L-1}'] - Y\n",
    "    grads[f'dW{L-1}'] = np.dot(dZ, cache[f'A{L-2}'].T) / m\n",
    "    grads[f'db{L-1}'] = np.sum(dZ, axis = 1, keepdims = True) / m \n",
    "\n",
    "    # for L-1 precendent layers, use the derivative of relu\n",
    "    for i in range(L-2, 0, -1):\n",
    "        dA = np.dot(parameters[f'W{i+1}'].T, dZ)\n",
    "        dZ = dA * relu(cache[f'Z{i}'], derivative=True)\n",
    "        grads[f'dW{i}'] = np.dot(dZ, cache[f'A{i-1}'].T) / m\n",
    "        grads[f'db{i}'] = np.sum(dZ, axis = 1, keepdims = True) / m\n",
    "    \n",
    "    return grads\n",
    "\n",
    "\n",
    "''' 4. update parameters - W & b'''\n",
    "\n",
    "# update parameters W & b using the gradients that were calculated from the backward pass\n",
    "def update_parameters(parameters, grads, learning_rate, layer_dims):\n",
    "    \n",
    "    for i in range(1, len(layer_dims)):\n",
    "        parameters[f'W{i}'] -= learning_rate * grads[f'dW{i}']\n",
    "        parameters[f'b{i}'] -= learning_rate * grads[f'db{i}']\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost0: 0.9142763543194594\n",
      "cost5000: 0.001379920320682325\n",
      "cost10000: 0.0006036301993199402\n",
      "cost15000: 0.0003765649114484425\n",
      "cost20000: 0.0002704438416725135\n",
      "cost25000: 0.0002095812832933118\n",
      "cost30000: 0.00017035274916517716\n",
      "cost35000: 0.00014306940874573944\n",
      "cost40000: 0.00012305186381462166\n",
      "cost45000: 0.0001077705853611852\n"
     ]
    }
   ],
   "source": [
    "''' training the model '''\n",
    "col_num = 10000\n",
    "layer_dims = 2, 5, 1\n",
    "learning_rate = 0.1\n",
    "iteration = 50000\n",
    "\n",
    "X, Y = create_dataset(col_num)\n",
    "parameters = initialize_parameters(layer_dims)\n",
    "\n",
    "for i in range(iteration):\n",
    "    \n",
    "    # 1. forward propagation\n",
    "    cache = forward_pass(X, parameters, layer_dims)\n",
    "    # 2. cost function\n",
    "    J = cost(cache[f'A{len(layer_dims)-1}'], Y)\n",
    "    # 3. backward propagation\n",
    "    grads = backward_pass(cache, parameters, Y, layer_dims)\n",
    "    # 4. update parameters\n",
    "    parameters = update_parameters(parameters, grads, learning_rate, layer_dims)\n",
    "    \n",
    "    if i % 5000 == 0:\n",
    "        print(f'cost{i}: {J}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' calculate accuracy '''\n",
    "\n",
    "def predict(col_num, X, Y, parameters, layer_nums):\n",
    "    # create test dataset\n",
    "    X_test, Y_test = create_dataset(col_num)\n",
    "\n",
    "    # calculate the 'prediction' of the train dataset\n",
    "    cache_train = forward_pass(X, parameters, layer_nums)\n",
    "    A_train = (cache_train[f'A{len(layer_dims)-1}'] > 0.5) * 1\n",
    "\n",
    "    # calculate the prediction of the test dataset\n",
    "    cache_test = forward_pass(X_test, parameters, layer_nums)\n",
    "    A_test = (cache_test[f'A{len(layer_dims)-1}'] > 0.5) * 1\n",
    "\n",
    "    # calculate the accuracy of the both datasets\n",
    "    accuracy_train = (A_train == Y) * 1\n",
    "    accuracy_test = (A_test == Y_test) * 1\n",
    "\n",
    "    # print out the results\n",
    "    print(f'accuracy_train: {np.average(accuracy_train) * 100}%')\n",
    "    print(f'accuracy_test: {np.average(accuracy_test) * 100}%')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_train: 99.98%\n",
      "accuracy_test: 99.94%\n"
     ]
    }
   ],
   "source": [
    "predict(col_num, X, Y, parameters, layer_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
